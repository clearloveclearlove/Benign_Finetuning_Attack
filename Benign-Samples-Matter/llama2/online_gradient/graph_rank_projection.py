"""
graph_greedy_selector.py
åŸºäºæ¢¯åº¦ååŒçš„å›¾è´ªå¿ƒæ ·æœ¬é€‰æ‹©ç®—æ³•

ç†è®ºåŸºç¡€ï¼š
- ç›®æ ‡å‡½æ•°ï¼šmax ||Î£ g_i|| ï¼ŒåŒæ—¶å¥–åŠ±ä¸ªä½“å¼ºåº¦å’Œå›¢é˜ŸååŒ
- ä¼˜åŒ–æ–¹æ³•ï¼šè´ªå¿ƒç®—æ³•ï¼Œæ¯æ­¥é€‰æ‹©å¸¦æ¥æœ€å¤§å¢ç›Šçš„æ ·æœ¬
- ç©ºé—´ä¼˜åŒ–ï¼šä½¿ç”¨éšæœºæŠ•å½±å‹ç¼©æ¢¯åº¦ (7Bç»´ â†’ 8Kç»´)
- ç†è®ºä¿è¯ï¼šJohnson-Lindenstrauss å¼•ç†ä¿è¯å‡ ä½•å…³ç³»
"""

import argparse
import json
import os
import sys
from typing import List, Dict

import fire
import torch
from tqdm import tqdm
from transformers import (AutoModelForCausalLM, LlamaTokenizer,
                          default_data_collator)

from configs.training import train_config
from utils.config_utils import generate_dataset_config, update_config
from utils.dataset_utils import get_preprocessed_dataset


class UltraMemoryEfficientProjector:
    """
    è¶…çº§å†…å­˜é«˜æ•ˆçš„éšæœºæŠ•å½±å™¨

    å…³é”®ä¼˜åŒ–ï¼š
    1. åœ¨CPUä¸Šè¿›è¡ŒæŠ•å½±è®¡ç®—
    2. ä½¿ç”¨éå¸¸å°çš„å—ï¼ˆåªéœ€å‡ MBï¼‰
    3. é€å—ç´¯ç§¯ç»“æœ
    """

    def __init__(self, grad_dim, proj_dim, seed=42):
        self.grad_dim = grad_dim
        self.proj_dim = proj_dim
        self.seed = seed
        self.scale = 1.0 / torch.sqrt(torch.tensor(proj_dim, dtype=torch.float32))

        print(f"   âœ“ ä½¿ç”¨CPUæŠ•å½± (é¿å…GPUæ˜¾å­˜é—®é¢˜)")

    def project(self, grad):
        """
        åœ¨CPUä¸ŠæŠ•å½±æ¢¯åº¦
        grad: (grad_dim,) tensor on GPU or CPU
        è¿”å›: (proj_dim,) tensor on CPU
        """
        # è½¬ç§»åˆ°CPU
        grad_cpu = grad.cpu().float()

        result = torch.zeros(self.proj_dim, dtype=torch.float32)

        # ä½¿ç”¨å¾ˆå°çš„å— (åªéœ€è¦ proj_dim * chunk_size * 4 bytes)
        # ä¾‹å¦‚ï¼š8192 * 10000 * 4 = 327 MB
        chunk_size = 10000
        num_chunks = (self.grad_dim + chunk_size - 1) // chunk_size

        for chunk_idx in range(num_chunks):
            start = chunk_idx * chunk_size
            end = min(start + chunk_size, self.grad_dim)
            chunk_len = end - start

            # ä¸ºè¿™ä¸ªå—ç”ŸæˆéšæœºæŠ•å½±çŸ©é˜µ
            torch.manual_seed(self.seed + chunk_idx)
            random_proj = torch.randint(
                0, 2, (self.proj_dim, chunk_len),
                dtype=torch.float32
            ) * 2.0 - 1.0  # {-1, +1}

            # æŠ•å½±è¿™ä¸ªå—
            grad_chunk = grad_cpu[start:end]
            result += torch.matmul(random_proj, grad_chunk)

        # å½’ä¸€åŒ–
        result = result * self.scale

        return result


def greedy_graph_selector(**kwargs):
    """
    å›¾ååŒè´ªå¿ƒæ ·æœ¬é€‰æ‹©ï¼ˆè¶…çº§å†…å­˜ä¼˜åŒ–ç‰ˆï¼‰
    """
    print("=" * 80)
    print("å›¾ååŒæ”»å‡»æ ·æœ¬é€‰æ‹© (è¶…çº§å†…å­˜ä¼˜åŒ–ç‰ˆ - CPUæŠ•å½±)")
    print("=" * 80)
    print()

    # ========== 1. é…ç½® ==========
    update_config((train_config,), **kwargs)

    torch.manual_seed(train_config.seed)
    torch.cuda.manual_seed_all(train_config.seed)

    k = kwargs.get("k", 100)
    num_candidates = kwargs.get("num_candidates", 1000)
    proj_dim = kwargs.get("proj_dim", 8192)
    normalize_grads = kwargs.get("normalize_grads", True)
    output_dir = kwargs.get("output_dir", "experiments/graph_attack/")
    dataset_name = kwargs.get("dataset_name", "graph_selected")

    os.makedirs(output_dir, exist_ok=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“ è®¡ç®—è®¾å¤‡: {device} (æ¨¡å‹)")
    print(f"ğŸ“ æŠ•å½±è®¾å¤‡: CPU (é¿å…æ˜¾å­˜é—®é¢˜)")

    # ========== 2. åŠ è½½æ¨¡å‹å’Œæ•°æ® ==========
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {train_config.model_name}")

    model = AutoModelForCausalLM.from_pretrained(
        train_config.model_name,
        torch_dtype=torch.bfloat16,
        device_map={"": device}
    )
    model.eval()

    tokenizer = LlamaTokenizer.from_pretrained(train_config.model_name)
    tokenizer.pad_token = tokenizer.eos_token

    print("ğŸ“š åŠ è½½æ•°æ®é›†...")
    dataset_config = generate_dataset_config(train_config, kwargs)
    full_dataset = get_preprocessed_dataset(tokenizer, dataset_config, split="train")

    actual_candidates = min(num_candidates, len(full_dataset))
    candidate_indices = list(range(actual_candidates))
    candidate_dataset = torch.utils.data.Subset(full_dataset, candidate_indices)

    print(f"\nâœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"âœ“ æ•°æ®é›†: {len(full_dataset):,} æ ·æœ¬")
    print(f"âœ“ å€™é€‰æ± : {actual_candidates:,} æ ·æœ¬")
    print(f"âœ“ ç›®æ ‡: é€‰æ‹© {k} ä¸ªæ ·æœ¬")

    # ========== 3. åˆå§‹åŒ–æŠ•å½±å™¨ ==========
    grad_dim = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ”§ åˆå§‹åŒ–æŠ•å½±å™¨")
    print(f"   åŸå§‹ç»´åº¦: {grad_dim:,}")
    print(f"   æŠ•å½±ç»´åº¦: {proj_dim:,}")
    print(f"   å‹ç¼©æ¯”: {grad_dim / proj_dim:.0f}x")

    projector = UltraMemoryEfficientProjector(
        grad_dim=grad_dim,
        proj_dim=proj_dim,
        seed=train_config.seed
    )

    # ========== 4. è®¡ç®—å¹¶æŠ•å½±æ¢¯åº¦ ==========
    print(f"\nğŸ§® è®¡ç®—æŠ•å½±æ¢¯åº¦...")
    print(f"   å€™é€‰æ ·æœ¬: {actual_candidates}")
    print(f"   é¢„è®¡æ—¶é—´: ~{actual_candidates * 0.5 / 60:.1f} åˆ†é’Ÿ")
    print()

    candidate_dataloader = torch.utils.data.DataLoader(
        candidate_dataset,
        batch_size=1,
        collate_fn=default_data_collator,
        pin_memory=True,
        num_workers=0,
    )

    projected_gradients = []
    original_norms = []

    for batch_idx, batch in enumerate(tqdm(candidate_dataloader, desc="æŠ•å½±æ¢¯åº¦")):
        batch = {key: val.to(device) for key, val in batch.items()}

        # å‰å‘+åå‘
        loss = model(**batch).loss
        loss.backward()

        with torch.no_grad():
            # æå–æ¢¯åº¦
            full_grad = torch.cat([
                p.grad.view(-1) for p in model.parameters()
                if p.grad is not None
            ])

            # ä¿å­˜èŒƒæ•°
            original_norm = torch.norm(full_grad).item()
            original_norms.append(original_norm)

            # å½’ä¸€åŒ–
            if normalize_grads:
                full_grad = full_grad / (original_norm + 1e-8)

            # CPUæŠ•å½±ï¼ˆè‡ªåŠ¨å¤„ç†è®¾å¤‡è½¬æ¢ï¼‰
            proj_grad = projector.project(full_grad)

            # æ¢å¤èŒƒæ•°
            if normalize_grads:
                proj_grad = proj_grad * original_norm

            projected_gradients.append(proj_grad)

        model.zero_grad()

        # å®šæœŸæ¸…ç†
        if (batch_idx + 1) % 50 == 0:
            torch.cuda.empty_cache()

    avg_norm = sum(original_norms) / len(original_norms)
    print(f"\nâœ“ æŠ•å½±å®Œæˆ")
    print(f"   å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_norm:.2f}")
    print(f"   å­˜å‚¨å¤§å°: ~{len(projected_gradients) * proj_dim * 4 / 1024 ** 2:.1f} MB")

    # é‡Šæ”¾æ¨¡å‹
    del model
    torch.cuda.empty_cache()

    # ========== 5. è´ªå¿ƒé€‰æ‹© ==========
    print(f"\nğŸ¯ è´ªå¿ƒé€‰æ‹©ç®—æ³•...")

    # é€‰æ‹©åœ¨å“ªä¸ªè®¾å¤‡ä¸Šè¿›è¡Œ
    # å¦‚æœGPUæœ‰è¶³å¤Ÿç©ºé—´ï¼Œç”¨GPUï¼›å¦åˆ™ç”¨CPU
    if torch.cuda.is_available():
        try:
            # å°è¯•åœ¨GPUä¸Š
            test_tensor = torch.zeros(proj_dim * len(projected_gradients), device='cuda')
            del test_tensor
            compute_device = torch.device('cuda')
            print(f"   åœ¨ GPU ä¸Šè¿›è¡Œé€‰æ‹©")
        except:
            compute_device = torch.device('cpu')
            print(f"   åœ¨ CPU ä¸Šè¿›è¡Œé€‰æ‹©")
    else:
        compute_device = torch.device('cpu')
        print(f"   åœ¨ CPU ä¸Šè¿›è¡Œé€‰æ‹©")

    projected_gradients_device = [g.to(compute_device) for g in projected_gradients]

    sum_of_grads = torch.zeros(proj_dim, device=compute_device, dtype=torch.float32)
    selected_indices = []
    available_indices = set(range(len(projected_gradients_device)))

    current_norm_sq = 0.0

    print()
    progress_bar = tqdm(range(k), desc="é€‰æ‹©æ ·æœ¬")
    for iteration in progress_bar:
        best_gain = -float('inf')
        best_idx = -1

        # æ‰¾æœ€ä½³å€™é€‰
        for idx in available_indices:
            grad = projected_gradients_device[idx]

            dot_product = torch.dot(sum_of_grads, grad)
            norm_sq = torch.dot(grad, grad)
            gain = (norm_sq + 2 * dot_product).item()

            if gain > best_gain:
                best_gain = gain
                best_idx = idx

        if best_idx == -1:
            break

        # æ›´æ–°
        selected_grad = projected_gradients_device[best_idx]
        sum_of_grads = sum_of_grads + selected_grad
        current_norm_sq += best_gain

        selected_indices.append(best_idx)
        available_indices.remove(best_idx)

        # æ›´æ–°è¿›åº¦
        if (iteration + 1) % 10 == 0:
            norm = torch.sqrt(torch.tensor(current_norm_sq))
            progress_bar.set_postfix({'åˆåŠ›èŒƒæ•°': f'{norm:.1f}'})

    final_norm = torch.sqrt(torch.tensor(current_norm_sq)).item()

    print(f"\nâœ“ é€‰æ‹©å®Œæˆ")
    print(f"   é€‰ä¸­æ ·æœ¬æ•°: {len(selected_indices)}")
    print(f"   åˆåŠ›æ¢¯åº¦èŒƒæ•°: {final_norm:.2f}")

    # ========== 6. ä¿å­˜ ==========
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœ...")

    final_indices = [candidate_indices[i] for i in selected_indices]

    with open(dataset_config.data_path, "r") as f:
        if dataset_config.data_path.endswith(".jsonl"):
            original_data = [json.loads(line) for line in f]
        else:
            original_data = json.load(f)

    selected_data = [original_data[i] for i in final_indices]

    output_file = os.path.join(output_dir, f"{dataset_name}_top{k}.json")
    with open(output_file, "w") as f:
        json.dump(selected_data, f, indent=4)

    info_file = os.path.join(output_dir, f"{dataset_name}_top{k}_info.json")
    with open(info_file, "w") as f:
        json.dump({
            "method": "graph_greedy_cpu_projection",
            "selected_count": len(selected_indices),
            "projection_dim": proj_dim,
            "final_norm": final_norm,
            "selected_indices": sorted(final_indices),
        }, f, indent=4)

    print(f"âœ“ {output_file}")
    print(f"âœ“ {info_file}")
    print("\n" + "=" * 80)
    print("å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    fire.Fire(greedy_graph_selector)